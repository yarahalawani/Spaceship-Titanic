{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>1. Raw Exploratory Data Analysis :</h1>","metadata":{}},{"cell_type":"code","source":"#exploring the data set before making any changes","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:57.246815Z","iopub.execute_input":"2024-04-02T22:46:57.247336Z","iopub.status.idle":"2024-04-02T22:46:57.254551Z","shell.execute_reply.started":"2024-04-02T22:46:57.247292Z","shell.execute_reply":"2024-04-02T22:46:57.253337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Importing necessary libraries :","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T22:46:57.261174Z","iopub.execute_input":"2024-04-02T22:46:57.261870Z","iopub.status.idle":"2024-04-02T22:46:58.634993Z","shell.execute_reply.started":"2024-04-02T22:46:57.261833Z","shell.execute_reply":"2024-04-02T22:46:58.633622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Loading Datasets :\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")\nsample_df = pd.read_csv(\"/kaggle/input/spaceship-titanic/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.636469Z","iopub.execute_input":"2024-04-02T22:46:58.637075Z","iopub.status.idle":"2024-04-02T22:46:58.715555Z","shell.execute_reply.started":"2024-04-02T22:46:58.637037Z","shell.execute_reply":"2024-04-02T22:46:58.714218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Checking dimensions of data:","metadata":{}},{"cell_type":"code","source":"print(\"Training Dataset shape is: \",train_df.shape)\nprint(\"Testing Dataset shape is: \",test_df.shape)\nprint(\"Sample Dataset shape is: \",sample_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.717255Z","iopub.execute_input":"2024-04-02T22:46:58.718353Z","iopub.status.idle":"2024-04-02T22:46:58.725244Z","shell.execute_reply.started":"2024-04-02T22:46:58.718317Z","shell.execute_reply":"2024-04-02T22:46:58.724034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Checking the first few rows of the dataset:","metadata":{}},{"cell_type":"code","source":"train_df.head(15)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.730205Z","iopub.execute_input":"2024-04-02T22:46:58.730776Z","iopub.status.idle":"2024-04-02T22:46:58.769802Z","shell.execute_reply.started":"2024-04-02T22:46:58.730714Z","shell.execute_reply":"2024-04-02T22:46:58.768504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* getting a quick overview of the features","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.771205Z","iopub.execute_input":"2024-04-02T22:46:58.771576Z","iopub.status.idle":"2024-04-02T22:46:58.797906Z","shell.execute_reply.started":"2024-04-02T22:46:58.771547Z","shell.execute_reply":"2024-04-02T22:46:58.796590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Checking the data types of each column:","metadata":{}},{"cell_type":"code","source":"train_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.800046Z","iopub.execute_input":"2024-04-02T22:46:58.800551Z","iopub.status.idle":"2024-04-02T22:46:58.810210Z","shell.execute_reply.started":"2024-04-02T22:46:58.800500Z","shell.execute_reply":"2024-04-02T22:46:58.809072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Checking for missing values","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.812057Z","iopub.execute_input":"2024-04-02T22:46:58.812472Z","iopub.status.idle":"2024-04-02T22:46:58.831904Z","shell.execute_reply.started":"2024-04-02T22:46:58.812440Z","shell.execute_reply":"2024-04-02T22:46:58.830309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Checking for unique values","metadata":{}},{"cell_type":"code","source":"train_df.nunique()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.833483Z","iopub.execute_input":"2024-04-02T22:46:58.833883Z","iopub.status.idle":"2024-04-02T22:46:58.860597Z","shell.execute_reply.started":"2024-04-02T22:46:58.833849Z","shell.execute_reply":"2024-04-02T22:46:58.859147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Exploring numerical features using summary statistics:","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.862533Z","iopub.execute_input":"2024-04-02T22:46:58.863768Z","iopub.status.idle":"2024-04-02T22:46:58.902187Z","shell.execute_reply.started":"2024-04-02T22:46:58.863718Z","shell.execute_reply":"2024-04-02T22:46:58.900554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Visualizing distributions of numerical features using histograms:","metadata":{}},{"cell_type":"code","source":"numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\nfor feature in numerical_features:\n    plt.figure(figsize=(6, 3))\n    plt.hist(train_df[feature], bins=20, color='skyblue', edgecolor='black')\n    plt.title(f'Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:46:58.904188Z","iopub.execute_input":"2024-04-02T22:46:58.904788Z","iopub.status.idle":"2024-04-02T22:47:00.451269Z","shell.execute_reply.started":"2024-04-02T22:46:58.904735Z","shell.execute_reply":"2024-04-02T22:47:00.449982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> The distributions indicate that directly incorporating these features into the model might compromise its performance. To address this, we may consider using only the algorithms that are unaffected by outliers","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15, 5))\n\nfor i, feature in enumerate(['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']):\n    sns.boxplot(x='Transported', y=feature, data=train_df, ax=axes[i])\n    axes[i].set_title(f'{feature} Distribution by Transported')\n    axes[i].set_xlabel('Transported')\n    axes[i].set_ylabel(feature)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:00.453430Z","iopub.execute_input":"2024-04-02T22:47:00.453961Z","iopub.status.idle":"2024-04-02T22:47:01.755153Z","shell.execute_reply.started":"2024-04-02T22:47:00.453913Z","shell.execute_reply":"2024-04-02T22:47:01.753906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> Indeed the continuous features exhibit skewness and contain outliers. Hence, we may explore techniques such as log transformations to address these issues.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\n\nsns.violinplot(data=train_df, x='Transported', y='Spa', palette='Set1')\nplt.title('Impact of Spa on Transported')\n\nplt.figure(figsize=(8, 6))\n\nsns.violinplot(data=train_df, x='Transported', y='VRDeck', palette='Set2')\nplt.title('Impact of VRDeck on Transported')\n\n\nplt.figure(figsize=(8, 6))\nsns.violinplot(data=train_df, x='Transported', y='RoomService', palette='Set3')\nplt.title('Impact of RoomService on Transported')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:01.757661Z","iopub.execute_input":"2024-04-02T22:47:01.758236Z","iopub.status.idle":"2024-04-02T22:47:02.920259Z","shell.execute_reply.started":"2024-04-02T22:47:01.758184Z","shell.execute_reply":"2024-04-02T22:47:02.918823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In these violin plots, The width of the plot at each value indicates the probability density of the data, providing insights into impact of each feature on predicting the target feature.","metadata":{}},{"cell_type":"markdown","source":"\n-> after reviewing the plots depicting the spending on Spa, VRDeck, and RoomService, it shows distinct separation between the classes, with individuals who spent less on these amenities being predominantly classified as Transported, suggesting the potential for creating a new feature representing total expenditure across these amenities.","metadata":{}},{"cell_type":"markdown","source":"* Visualizing categorical features distributions :","metadata":{}},{"cell_type":"code","source":"categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n\nfor feature in categorical_features:\n    plt.figure(figsize=(6, 4))\n    sns.countplot(data=train_df, x=feature, palette='viridis')\n    plt.title(f'Count Plot of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)  \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:02.922584Z","iopub.execute_input":"2024-04-02T22:47:02.923128Z","iopub.status.idle":"2024-04-02T22:47:03.744200Z","shell.execute_reply.started":"2024-04-02T22:47:02.923083Z","shell.execute_reply":"2024-04-02T22:47:03.742838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now let's visualize the relationship between categorical features and the target feature \"transported\" using count plots :","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncategorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n\nfor feature in categorical_features:\n    plt.figure(figsize=(8, 6))\n    sns.countplot(data=train_df, x=feature, hue='Transported', palette='viridis')\n    plt.title(f'Count Plot of {feature} vs Transported')\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)  \n    plt.legend(title='Transported', loc='upper right')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:03.746094Z","iopub.execute_input":"2024-04-02T22:47:03.747537Z","iopub.status.idle":"2024-04-02T22:47:04.877898Z","shell.execute_reply.started":"2024-04-02T22:47:03.747486Z","shell.execute_reply":"2024-04-02T22:47:04.876498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> Cryosleep has a good difference in proportions, poeple who are in Cryosleep  during the voyage are more likely to be Transported","metadata":{}},{"cell_type":"code","source":"categorical_features = ['PassengerId','HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP','Name']\n\nfor feature in categorical_features:\n    cardinality = train_df[feature].nunique()\n    print(f\"Cardinality of '{feature}': {cardinality}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:04.879469Z","iopub.execute_input":"2024-04-02T22:47:04.879888Z","iopub.status.idle":"2024-04-02T22:47:04.898046Z","shell.execute_reply.started":"2024-04-02T22:47:04.879851Z","shell.execute_reply":"2024-04-02T22:47:04.897108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Checking the distribution of the target variable :","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4, 2))\nsns.countplot(data=train_df, x='Transported', palette='Set3')\nplt.title('Distribution of Transported')\nplt.xlabel('Transported')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:04.905087Z","iopub.execute_input":"2024-04-02T22:47:04.905553Z","iopub.status.idle":"2024-04-02T22:47:05.072200Z","shell.execute_reply.started":"2024-04-02T22:47:04.905516Z","shell.execute_reply":"2024-04-02T22:47:05.071121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> since the data is balanced, accuracy is a suitable metric for evaluation.\nbecause accuracy measures the overall correctness of the model by considering TP and TN.","metadata":{}},{"cell_type":"markdown","source":"<h1>2. Data Preprocessing:</h1>","metadata":{}},{"cell_type":"code","source":"#for convience we are going to apply preprocessiing on train_df and test_df in the same time","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:05.073686Z","iopub.execute_input":"2024-04-02T22:47:05.075034Z","iopub.status.idle":"2024-04-02T22:47:05.079839Z","shell.execute_reply.started":"2024-04-02T22:47:05.074996Z","shell.execute_reply":"2024-04-02T22:47:05.078561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Missing values:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate percentage of missing values in each feature\nmissing_percentage = train_df.isnull().mean() * 100\n\n# Plotting the percentage of missing values in each feature\nplt.figure(figsize=(4, 3))\nsns.barplot(x=missing_percentage.values, y=missing_percentage.index, palette='viridis')\nplt.xlabel('Percentage of Missing Values')\nplt.ylabel('Features')\nplt.title('Percentage of Missing Values in Each Feature')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:05.081596Z","iopub.execute_input":"2024-04-02T22:47:05.082394Z","iopub.status.idle":"2024-04-02T22:47:05.404180Z","shell.execute_reply.started":"2024-04-02T22:47:05.082343Z","shell.execute_reply":"2024-04-02T22:47:05.403065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"->PassengerId and transported don't have any missing values. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 3))\nsns.heatmap(train_df.isnull(), cmap='viridis', cbar=False)\nplt.title('Missing Value Distribution')\nplt.show()\n\nplt.figure(figsize=(6, 3))\nsns.heatmap(train_df.isnull().corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation between Missing Values')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:05.406023Z","iopub.execute_input":"2024-04-02T22:47:05.406712Z","iopub.status.idle":"2024-04-02T22:47:06.660638Z","shell.execute_reply.started":"2024-04-02T22:47:05.406674Z","shell.execute_reply":"2024-04-02T22:47:06.659505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> Missing values are independent of the target and for the most part are isolated. Even though only 2% of the data is missing, about 25% of all passengers have at least 1 missing value","metadata":{}},{"cell_type":"markdown","source":"* ***Handling missing values: imputing***","metadata":{}},{"cell_type":"markdown","source":"-> it is reasonable to attempt to fill in these missing values rather than simply discarding rows.","metadata":{}},{"cell_type":"markdown","source":"* Separating the numerical and nominal attributes ","metadata":{}},{"cell_type":"code","source":"\n\nnumerical_features =train_df.select_dtypes(include=[np.number])\nnominal_features = train_df.select_dtypes(exclude=[np.number])\n\ntest_numerical_features =test_df.select_dtypes(include=[np.number])\ntest_nominal_features = test_df.select_dtypes(exclude=[np.number])\n\nprint(\"Nombre de colonnes numériques :\", numerical_features.shape[1])\nprint(\"Nombre de colonnes non numériques (catégorielles) :\", nominal_features.shape[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:06.662761Z","iopub.execute_input":"2024-04-02T22:47:06.663463Z","iopub.status.idle":"2024-04-02T22:47:06.675127Z","shell.execute_reply.started":"2024-04-02T22:47:06.663427Z","shell.execute_reply":"2024-04-02T22:47:06.673693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* applying KNN imputer because it can capture more complex patterns in the data compared to simple imputation methods like median or mean imputation :","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=5)  \n\nnumerical_features_imputed = pd.DataFrame(knn_imputer.fit_transform(numerical_features), columns=numerical_features.columns)\ntest_numerical_features_imputed = pd.DataFrame(knn_imputer.fit_transform(test_numerical_features), columns=test_numerical_features.columns)\n\n\nprint(numerical_features_imputed.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:06.676629Z","iopub.execute_input":"2024-04-02T22:47:06.677057Z","iopub.status.idle":"2024-04-02T22:47:07.793214Z","shell.execute_reply.started":"2024-04-02T22:47:06.677024Z","shell.execute_reply":"2024-04-02T22:47:07.791828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import IterativeImputer\n\nimputer = IterativeImputer(random_state=0)\n\nnumerical_features_imputed = pd.DataFrame(imputer.fit_transform(numerical_features), columns=numerical_features.columns)\ntest_numerical_features_imputed = pd.DataFrame(imputer.fit_transform(test_numerical_features), columns=test_numerical_features.columns)\nprint(numerical_features_imputed.isnull().sum())\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:07.794686Z","iopub.execute_input":"2024-04-02T22:47:07.795109Z","iopub.status.idle":"2024-04-02T22:47:07.803369Z","shell.execute_reply.started":"2024-04-02T22:47:07.795075Z","shell.execute_reply":"2024-04-02T22:47:07.802252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n \nnom_imputer = SimpleImputer(strategy=\"most_frequent\")\n\nnom_imputer.fit(nominal_features)\n\nnominal_features_imputed = pd.DataFrame(nom_imputer.transform(nominal_features), columns = nominal_features.columns)\n\nnominal_features_imputed.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:07.805817Z","iopub.execute_input":"2024-04-02T22:47:07.806735Z","iopub.status.idle":"2024-04-02T22:47:07.862100Z","shell.execute_reply.started":"2024-04-02T22:47:07.806615Z","shell.execute_reply":"2024-04-02T22:47:07.860767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" \ntest_nom_imputer = SimpleImputer(strategy=\"most_frequent\")\n \nnom_imputer.fit(test_nominal_features)\n \ntest_nominal_features_imputed = pd.DataFrame(nom_imputer.transform(test_nominal_features), columns = test_nominal_features.columns)\n ","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:07.863853Z","iopub.execute_input":"2024-04-02T22:47:07.864891Z","iopub.status.idle":"2024-04-02T22:47:07.888063Z","shell.execute_reply.started":"2024-04-02T22:47:07.864842Z","shell.execute_reply":"2024-04-02T22:47:07.886897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.concat((numerical_features_imputed, nominal_features_imputed), axis = 1)\ntest_df = pd.concat((test_numerical_features_imputed, test_nominal_features_imputed), axis = 1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:07.889734Z","iopub.execute_input":"2024-04-02T22:47:07.890276Z","iopub.status.idle":"2024-04-02T22:47:07.903669Z","shell.execute_reply.started":"2024-04-02T22:47:07.890231Z","shell.execute_reply":"2024-04-02T22:47:07.902509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**** Feature engineering: Creating new features***","metadata":{}},{"cell_type":"markdown","source":"-> creating family size feature using the last name from name feature :","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Extract family name from Name column\ntrain_df['FamilyName'] = train_df['Name'].str.split(' ').str[-1]\ntest_df['FamilyName'] = test_df['Name'].str.split(' ').str[-1]\n\n# Count occurrences of each family name to determine family size\ntrain_df['FamilySize'] = train_df.groupby('FamilyName')['FamilyName'].transform('count').astype(float)\ntest_df['FamilySize'] = test_df.groupby('FamilyName')['FamilyName'].transform('count').astype(float)\n\n# Count the number of unique family names\nnum_family_names = train_df['FamilyName'].nunique()\n\n# Plot the distribution of family size\nplt.figure(figsize=(8, 6))\nsns.countplot(data=train_df, x='FamilySize')\nplt.xlabel('Family Size')\nplt.ylabel('Count')\nplt.title('Distribution of Family Size')\nplt.show()\n\nprint(\"Number of unique family names:\", num_family_names)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:07.906849Z","iopub.execute_input":"2024-04-02T22:47:07.907405Z","iopub.status.idle":"2024-04-02T22:47:08.286490Z","shell.execute_reply.started":"2024-04-02T22:47:07.907361Z","shell.execute_reply":"2024-04-02T22:47:08.285147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nsns.countplot(data=train_df , x='FamilySize', hue='Transported')\n\nplt.xlabel('Family Size')\nplt.ylabel('Proportion of Transported')\nplt.title('Impact of Family Size on Transported Feature')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:08.288068Z","iopub.execute_input":"2024-04-02T22:47:08.288485Z","iopub.status.idle":"2024-04-02T22:47:08.947450Z","shell.execute_reply.started":"2024-04-02T22:47:08.288449Z","shell.execute_reply":"2024-04-02T22:47:08.946084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop('FamilyName', axis=1, inplace=True)\ntrain_df.drop('Name', axis=1, inplace=True)\ntest_df.drop('FamilyName', axis=1, inplace=True)\ntest_df.drop('Name', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:08.949771Z","iopub.execute_input":"2024-04-02T22:47:08.950302Z","iopub.status.idle":"2024-04-02T22:47:08.967660Z","shell.execute_reply.started":"2024-04-02T22:47:08.950251Z","shell.execute_reply":"2024-04-02T22:47:08.966531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> creating new age groupes features from age feature :","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_df['Age_group']=np.nan\ntrain_df.loc[train_df['Age']<=12,'Age_group']='Age_0-12'\ntrain_df.loc[(train_df['Age']>12) & (train_df['Age']<18),'Age_group']='Age_13-17'\ntrain_df.loc[(train_df['Age']>=18) & (train_df['Age']<=25),'Age_group']='Age_18-25'\ntrain_df.loc[(train_df['Age']>25) & (train_df['Age']<=30),'Age_group']='Age_26-30'\ntrain_df.loc[(train_df['Age']>30) & (train_df['Age']<=50),'Age_group']='Age_31-50'\ntrain_df.loc[train_df['Age']>50,'Age_group']='Age_51+'\n\ntest_df['Age_group']=np.nan\ntest_df.loc[test_df['Age']<=12,'Age_group']='Age_0-12'\ntest_df.loc[(test_df['Age']>12) & (test_df['Age']<18),'Age_group']='Age_13-17'\ntest_df.loc[(test_df['Age']>=18) & (test_df['Age']<=25),'Age_group']='Age_18-25'\ntest_df.loc[(test_df['Age']>25) & (test_df['Age']<=30),'Age_group']='Age_26-30'\ntest_df.loc[(test_df['Age']>30) & (test_df['Age']<=50),'Age_group']='Age_31-50'\ntest_df.loc[test_df['Age']>50,'Age_group']='Age_51+'\n\n\n# Plot distribution of new features\nplt.figure(figsize=(10, 4))\nsns.countplot(data=train_df, x='Age_group', hue='Transported', order=['Age_0-12', 'Age_13-17', 'Age_18-25', 'Age_26-30', 'Age_31-50', 'Age_51+'])\nplt.title('Age group distribution')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:08.969343Z","iopub.execute_input":"2024-04-02T22:47:08.970214Z","iopub.status.idle":"2024-04-02T22:47:09.315782Z","shell.execute_reply.started":"2024-04-02T22:47:08.970164Z","shell.execute_reply":"2024-04-02T22:47:09.314414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. -> this helps simplify data, handle outliers, improve interpretability, and potentially enhance model performance\n1. -> we can clearly see that passengers aged between 31 and 50 are the most transported ","metadata":{}},{"cell_type":"code","source":"#Identifying the condition for zerospending based on the sum of all Expenditure features\nexp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\ntrain_df['Total_Expenditure']=train_df[exp_feats].sum(axis=1)\ntrain_df['No_spending']=(train_df['Total_Expenditure']==0).astype(float)\n\ntest_df['Total_Expenditure']=test_df[exp_feats].sum(axis=1)\ntest_df['No_spending']=(test_df['Total_Expenditure']==0).astype(float)\n\nfig=plt.figure(figsize=(6,4))\nsns.countplot(data=train_df, x='No_spending', hue='Transported')\nplt.title('No spending indicator')\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:09.317710Z","iopub.execute_input":"2024-04-02T22:47:09.318268Z","iopub.status.idle":"2024-04-02T22:47:09.653078Z","shell.execute_reply.started":"2024-04-02T22:47:09.318222Z","shell.execute_reply":"2024-04-02T22:47:09.651731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extracting the groupe size feature from groups in passenger id \ntrain_df['Group'] = train_df['PassengerId'].apply(lambda x: x.split('_')[0]).astype(float)\ntrain_df['Group_size']=train_df['Group'].map(lambda x: train_df['Group'].value_counts()[x]).astype(float)\n\ntest_df['Group'] = test_df['PassengerId'].apply(lambda x: x.split('_')[0]).astype(float)\ntest_df['Group_size']=test_df['Group'].map(lambda x: test_df['Group'].value_counts()[x]).astype(float)\n\nfig=plt.figure(figsize=(6,4))\nsns.countplot(data=train_df, x='Group_size', hue='Transported')\nplt.title('Group size')\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:09.655155Z","iopub.execute_input":"2024-04-02T22:47:09.655956Z","iopub.status.idle":"2024-04-02T22:47:23.708943Z","shell.execute_reply.started":"2024-04-02T22:47:09.655911Z","shell.execute_reply":"2024-04-02T22:47:23.707443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> people in smaller groups are more transported ","metadata":{}},{"cell_type":"code","source":"# Creating Solo Traveling feature\ntrain_df['Solo_Traveling'] = (train_df['Group_size'] == 1).astype(float)\ntrain_df['Solo_Traveling'].head()\n\n# Creating Solo Traveling feature\ntest_df['Solo_Traveling'] = (test_df['Group_size'] == 1).astype(float)\ntest_df['Solo_Traveling'].head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:23.710712Z","iopub.execute_input":"2024-04-02T22:47:23.711129Z","iopub.status.idle":"2024-04-02T22:47:23.725451Z","shell.execute_reply.started":"2024-04-02T22:47:23.711096Z","shell.execute_reply":"2024-04-02T22:47:23.724134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. \nbut we will drop groupe number because of its big cardinality\n","metadata":{}},{"cell_type":"code","source":"train_df.drop('PassengerId', axis=1, inplace=True)\ntrain_df.drop('Group', axis=1, inplace=True)\n\ntest_df.drop('PassengerId', axis=1, inplace=True)\ntest_df.drop('Group', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:23.727309Z","iopub.execute_input":"2024-04-02T22:47:23.727703Z","iopub.status.idle":"2024-04-02T22:47:23.747076Z","shell.execute_reply.started":"2024-04-02T22:47:23.727673Z","shell.execute_reply":"2024-04-02T22:47:23.745818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new features\ntrain_df['Cabin_deck'] = train_df['Cabin'].apply(lambda x: x.split('/')[0])\ntrain_df['Cabin_number'] = train_df['Cabin'].apply(lambda x: x.split('/')[1] if '/' in x else np.nan).astype(float)\ntrain_df['Cabin_side'] = train_df['Cabin'].apply(lambda x: x.split('/')[2] if '/' in x else np.nan)\n\ntest_df['Cabin_deck'] = test_df['Cabin'].apply(lambda x: x.split('/')[0])\ntest_df['Cabin_number'] = test_df['Cabin'].apply(lambda x: x.split('/')[1] if '/' in x else np.nan).astype(float)\ntest_df['Cabin_side'] = test_df['Cabin'].apply(lambda x: x.split('/')[2] if '/' in x else np.nan)\n\n# Drop 'Cabin' column\ntrain_df.drop('Cabin', axis=1, inplace=True)\ntest_df.drop('Cabin', axis=1, inplace=True)\n\n# Plot distribution of new features\nfig = plt.figure(figsize=(5, 5))\nplt.subplot(2, 1, 1)\nsns.countplot(data=train_df, x='Cabin_deck', hue='Transported', order=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'])\nplt.title('Cabin deck')\n\nplt.subplot(2, 1, 2)\nsns.countplot(data=train_df, x='Cabin_side', hue='Transported')\nplt.title('Cabin side')\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:23.749169Z","iopub.execute_input":"2024-04-02T22:47:23.749797Z","iopub.status.idle":"2024-04-02T22:47:24.325491Z","shell.execute_reply.started":"2024-04-02T22:47:23.749737Z","shell.execute_reply":"2024-04-02T22:47:24.324166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New features - training set\ntrain_df['Cabin_region1'] = (train_df['Cabin_number'] < 300).astype(float)   # one-hot encoding\ntrain_df['Cabin_region2'] = ((train_df['Cabin_number'] >= 300) & (train_df['Cabin_number'] < 600)).astype(float)\ntrain_df['Cabin_region3'] = ((train_df['Cabin_number'] >= 600) & (train_df['Cabin_number'] < 900)).astype(float)\ntrain_df['Cabin_region4'] = ((train_df['Cabin_number'] >= 900) & (train_df['Cabin_number'] < 1200)).astype(float)\ntrain_df['Cabin_region5'] = ((train_df['Cabin_number'] >= 1200) & (train_df['Cabin_number'] < 1500)).astype(float)\ntrain_df['Cabin_region6'] = ((train_df['Cabin_number'] >= 1500) & (train_df['Cabin_number'] < 1800)).astype(float)\ntrain_df['Cabin_region7'] = (train_df['Cabin_number'] >= 1800).astype(float)\n\ntest_df['Cabin_region1'] = (test_df['Cabin_number'] < 300).astype(float)   # one-hot encoding\ntest_df['Cabin_region2'] = ((test_df['Cabin_number'] >= 300) & (test_df['Cabin_number'] < 600)).astype(float)\ntest_df['Cabin_region3'] = ((test_df['Cabin_number'] >= 600) & (test_df['Cabin_number'] < 900)).astype(float)\ntest_df['Cabin_region4'] = ((test_df['Cabin_number'] >= 900) & (test_df['Cabin_number'] < 1200)).astype(float)\ntest_df['Cabin_region5'] = ((test_df['Cabin_number'] >= 1200) & (test_df['Cabin_number'] < 1500)).astype(float)\ntest_df['Cabin_region6'] = ((test_df['Cabin_number'] >= 1500) & (test_df['Cabin_number'] < 1800)).astype(float)\ntest_df['Cabin_region7'] = (test_df['Cabin_number'] >= 1800).astype(float)\n\n# Plot distribution of new features\nplt.figure(figsize=(10, 4))\ntrain_df['Cabin_regions_plot'] = (train_df['Cabin_region1'] + 2 * train_df['Cabin_region2'] + 3 * train_df['Cabin_region3'] +\n                                  4 * train_df['Cabin_region4'] + 5 * train_df['Cabin_region5'] + 6 * train_df['Cabin_region6'] +\n                                  7 * train_df['Cabin_region7']).astype(float)\nsns.countplot(data=train_df, x='Cabin_regions_plot', hue='Transported')\nplt.title('Cabin regions')\ntrain_df.drop('Cabin_regions_plot', axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.327764Z","iopub.execute_input":"2024-04-02T22:47:24.328161Z","iopub.status.idle":"2024-04-02T22:47:24.762501Z","shell.execute_reply.started":"2024-04-02T22:47:24.328127Z","shell.execute_reply":"2024-04-02T22:47:24.760905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The location of the individual cabins also had an impact on the transportation of people.\n","metadata":{}},{"cell_type":"markdown","source":"-> checking the features that we added:","metadata":{}},{"cell_type":"code","source":"train_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.764205Z","iopub.execute_input":"2024-04-02T22:47:24.764611Z","iopub.status.idle":"2024-04-02T22:47:24.778456Z","shell.execute_reply.started":"2024-04-02T22:47:24.764578Z","shell.execute_reply":"2024-04-02T22:47:24.776920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.780791Z","iopub.execute_input":"2024-04-02T22:47:24.781462Z","iopub.status.idle":"2024-04-02T22:47:24.795263Z","shell.execute_reply.started":"2024-04-02T22:47:24.781388Z","shell.execute_reply":"2024-04-02T22:47:24.794002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> Data type separation, Transformation, Encoding, and Scaling</h1>","metadata":{}},{"cell_type":"code","source":"# Separating the numerical and nominal attributes again after future engineering \nnew_numerical_features =train_df.select_dtypes(include=[np.number])\nnew_nominal_features = train_df.select_dtypes(exclude=[np.number])\n\nnew_test_numerical_features =test_df.select_dtypes(include=[np.number])\nnew_test_nominal_features = test_df.select_dtypes(exclude=[np.number])\n\nprint(\"numériques :\", new_numerical_features.shape[1])\nprint(\"catégorielles :\", new_nominal_features.shape[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.796727Z","iopub.execute_input":"2024-04-02T22:47:24.797205Z","iopub.status.idle":"2024-04-02T22:47:24.815093Z","shell.execute_reply.started":"2024-04-02T22:47:24.797161Z","shell.execute_reply":"2024-04-02T22:47:24.813434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_nominal_features.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.816880Z","iopub.execute_input":"2024-04-02T22:47:24.817436Z","iopub.status.idle":"2024-04-02T22:47:24.828411Z","shell.execute_reply.started":"2024-04-02T22:47:24.817387Z","shell.execute_reply":"2024-04-02T22:47:24.827178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.830018Z","iopub.execute_input":"2024-04-02T22:47:24.830416Z","iopub.status.idle":"2024-04-02T22:47:24.869214Z","shell.execute_reply.started":"2024-04-02T22:47:24.830386Z","shell.execute_reply":"2024-04-02T22:47:24.867776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying log transformation to numerical features \nfor feature in new_numerical_features.columns:\n    train_df[feature] = np.log1p(train_df[feature].values)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.870808Z","iopub.execute_input":"2024-04-02T22:47:24.871189Z","iopub.status.idle":"2024-04-02T22:47:24.911283Z","shell.execute_reply.started":"2024-04-02T22:47:24.871159Z","shell.execute_reply":"2024-04-02T22:47:24.910041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in new_test_numerical_features.columns:\n    test_df[feature] = np.log1p(test_df[feature].values)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.912814Z","iopub.execute_input":"2024-04-02T22:47:24.913194Z","iopub.status.idle":"2024-04-02T22:47:24.952906Z","shell.execute_reply.started":"2024-04-02T22:47:24.913165Z","shell.execute_reply":"2024-04-02T22:47:24.951584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Encoding categorical variables: Using label encoding to convert categorical variables into numerical format ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\n# Encoding categorical features using LabelEncoder\nfor feature in new_nominal_features:\n    train_df[feature] = label_encoder.fit_transform(train_df[feature])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:24.954914Z","iopub.execute_input":"2024-04-02T22:47:24.955406Z","iopub.status.idle":"2024-04-02T22:47:25.017116Z","shell.execute_reply.started":"2024-04-02T22:47:24.955363Z","shell.execute_reply":"2024-04-02T22:47:25.015676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.018905Z","iopub.execute_input":"2024-04-02T22:47:25.023113Z","iopub.status.idle":"2024-04-02T22:47:25.055721Z","shell.execute_reply.started":"2024-04-02T22:47:25.023058Z","shell.execute_reply":"2024-04-02T22:47:25.054451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntest_label_encoder = LabelEncoder()\n\nfor feature in new_test_nominal_features:\n    test_df[feature] = label_encoder.fit_transform(test_df[feature])\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.057439Z","iopub.execute_input":"2024-04-02T22:47:25.057854Z","iopub.status.idle":"2024-04-02T22:47:25.108159Z","shell.execute_reply.started":"2024-04-02T22:47:25.057820Z","shell.execute_reply":"2024-04-02T22:47:25.106748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> checking coorelation after encoding categorical features:\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Spliting the data into features (X) and the target variable (y):","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(columns=['Transported'])   \ny = train_df['Transported']  \n\n# Printing to verify the split\nprint(\"Shape of X:\", X.shape)\nprint(\"Shape of y:\", y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.109983Z","iopub.execute_input":"2024-04-02T22:47:25.110977Z","iopub.status.idle":"2024-04-02T22:47:25.121222Z","shell.execute_reply.started":"2024-04-02T22:47:25.110933Z","shell.execute_reply":"2024-04-02T22:47:25.120009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest_df = scaler.fit_transform(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.123225Z","iopub.execute_input":"2024-04-02T22:47:25.124020Z","iopub.status.idle":"2024-04-02T22:47:25.153527Z","shell.execute_reply.started":"2024-04-02T22:47:25.123975Z","shell.execute_reply":"2024-04-02T22:47:25.152489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying PCA\nfrom sklearn.decomposition import PCA\n\n# Applying PCA\npca = PCA(n_components=0.95)  # Keep 95% of the variance\nX_pca = pca.fit_transform(X)\n\n# Check the shape of the transformed data\nprint(\"Shape of X after PCA:\", X_pca.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.155562Z","iopub.execute_input":"2024-04-02T22:47:25.156055Z","iopub.status.idle":"2024-04-02T22:47:25.188819Z","shell.execute_reply.started":"2024-04-02T22:47:25.156007Z","shell.execute_reply":"2024-04-02T22:47:25.187047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>3. Model Selection:</h1>","metadata":{}},{"cell_type":"code","source":"#spliting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.191495Z","iopub.execute_input":"2024-04-02T22:47:25.192909Z","iopub.status.idle":"2024-04-02T22:47:25.221346Z","shell.execute_reply.started":"2024-04-02T22:47:25.192834Z","shell.execute_reply":"2024-04-02T22:47:25.219572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 1) Logistic-Regression. </h2>","metadata":{}},{"cell_type":"code","source":"#Instantiate the Model\nlogistic_regression_model = LogisticRegression(max_iter=10000)\n\n#Fit the Model\nlogistic_regression_model.fit(X_train, y_train)\n\n#Make Predictions\ny_pred = logistic_regression_model.predict(X_test)\n\n#Evaluating the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.223532Z","iopub.execute_input":"2024-04-02T22:47:25.224396Z","iopub.status.idle":"2024-04-02T22:47:25.296674Z","shell.execute_reply.started":"2024-04-02T22:47:25.224341Z","shell.execute_reply":"2024-04-02T22:47:25.294601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 2) Support Vector Machine. </h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Instantiate the Model\nsvm_model = SVC()\n\n# Step 4: Fit the Model\nsvm_model.fit(X_train, y_train)\n\n# Step 5: Make Predictions\ny_pred = svm_model.predict(X_test)\n\n# Step 6: Evaluate the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Optionally, print classification report for more detailed evaluation\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:25.299021Z","iopub.execute_input":"2024-04-02T22:47:25.299983Z","iopub.status.idle":"2024-04-02T22:47:27.738200Z","shell.execute_reply.started":"2024-04-02T22:47:25.299934Z","shell.execute_reply":"2024-04-02T22:47:27.736731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 3) Naive Bayes. </h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n#   Instantiate the Model\nnaive_bayes_model = GaussianNB()\n\n# Step 4: Fit the Model\nnaive_bayes_model.fit(X_train, y_train)\n\n# Step 5: Make Predictions\ny_pred = naive_bayes_model.predict(X_test)\n\n# Step 6: Evaluate the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Optionally, print classification report for more detailed evaluation\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:27.750731Z","iopub.execute_input":"2024-04-02T22:47:27.751485Z","iopub.status.idle":"2024-04-02T22:47:27.786276Z","shell.execute_reply.started":"2024-04-02T22:47:27.751445Z","shell.execute_reply":"2024-04-02T22:47:27.784542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tree based ML models :","metadata":{}},{"cell_type":"markdown","source":"<h2> 4) Decision trees </h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n#  Instantiate the Model\ndecision_tree_model = DecisionTreeClassifier()\n\n# Step 4: Fit the Model\ndecision_tree_model.fit(X_train, y_train)\n\n# Step 5: Make Predictions\ny_pred = decision_tree_model.predict(X_test)\n\n# Step 6: Evaluate the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Optionally, print classification report for more detailed evaluation\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:27.787988Z","iopub.execute_input":"2024-04-02T22:47:27.788951Z","iopub.status.idle":"2024-04-02T22:47:27.882558Z","shell.execute_reply.started":"2024-04-02T22:47:27.788911Z","shell.execute_reply":"2024-04-02T22:47:27.881071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 5) ADA Boost</h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# Instantiate the Model\nadaboost_model = AdaBoostClassifier()\n\n# Step 4: Fit the Model\nadaboost_model.fit(X_train, y_train)\n\n# Step 5: Make Predictions\ny_pred = adaboost_model.predict(X_test)\n\n# Step 6: Evaluate the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Optionally, print classification report for more detailed evaluation\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:27.884494Z","iopub.execute_input":"2024-04-02T22:47:27.885018Z","iopub.status.idle":"2024-04-02T22:47:28.470108Z","shell.execute_reply.started":"2024-04-02T22:47:27.884975Z","shell.execute_reply":"2024-04-02T22:47:28.468723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 6) gradient Boost</h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\n#  Instantiate the Model\ngradientboost_model = GradientBoostingClassifier()\n\n# Step 4: Fit the Model\ngradientboost_model.fit(X_train, y_train)\n\n# Step 5: Make Predictions\ny_pred = gradientboost_model.predict(X_test)\n\n# Step 6: Evaluate the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Optionally, print classification report for more detailed evaluation\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:28.472062Z","iopub.execute_input":"2024-04-02T22:47:28.472573Z","iopub.status.idle":"2024-04-02T22:47:30.040315Z","shell.execute_reply.started":"2024-04-02T22:47:28.472529Z","shell.execute_reply":"2024-04-02T22:47:30.039025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 7) Random Forest</h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Instantiate the Model\nrandomforest_model = RandomForestClassifier()\n\n# Step 4: Fit the Model\nrandomforest_model.fit(X_train, y_train)\n\n# Step 5: Make Predictions\ny_pred = randomforest_model.predict(X_test)\n\n# Step 6: Evaluate the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Optionally, print classification report for more detailed evaluation\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:30.042159Z","iopub.execute_input":"2024-04-02T22:47:30.042666Z","iopub.status.idle":"2024-04-02T22:47:31.448657Z","shell.execute_reply.started":"2024-04-02T22:47:30.042603Z","shell.execute_reply":"2024-04-02T22:47:31.447471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>8) XGBoost</h2>","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nxgboost_model = xgb.XGBClassifier()\n\nxgboost_model.fit(X_train, y_train)\n\ny_pred = xgboost_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:31.450370Z","iopub.execute_input":"2024-04-02T22:47:31.450820Z","iopub.status.idle":"2024-04-02T22:47:31.699103Z","shell.execute_reply.started":"2024-04-02T22:47:31.450774Z","shell.execute_reply":"2024-04-02T22:47:31.697792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<h2>9) LightGBM:</h2>","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\n# Initialize the LightGBM classifier\nlightgbm_model = lgb.LGBMClassifier(verbose=0)\n\n# Train the model\nlightgbm_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = lightgbm_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:31.705021Z","iopub.execute_input":"2024-04-02T22:47:31.708140Z","iopub.status.idle":"2024-04-02T22:47:33.451857Z","shell.execute_reply.started":"2024-04-02T22:47:31.708091Z","shell.execute_reply":"2024-04-02T22:47:33.450792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>10) CatBoost:</h2>","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n# Initialize the CatBoost classifier with verbose set to False\ncatboost_model = CatBoostClassifier(verbose=False)\n\n# Train the model\ncatboost_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = catboost_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred) \n\nprint(\"Accuracy:\", accuracy) ","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:33.453116Z","iopub.execute_input":"2024-04-02T22:47:33.454079Z","iopub.status.idle":"2024-04-02T22:47:38.062278Z","shell.execute_reply.started":"2024-04-02T22:47:33.454044Z","shell.execute_reply":"2024-04-02T22:47:38.061289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Define a dictionary to hold the models\nmodels = {\n    'Logistic Regression': logistic_regression_model,\n    'Support Vector Machine': svm_model,\n    'Naive Bayes': naive_bayes_model,\n    'Decision Trees': decision_tree_model,\n    'AdaBoost': adaboost_model,\n    'Gradient Boost': gradientboost_model,\n    'Random Forest': randomforest_model,\n    'XGBoost': xgboost_model,\n    'LightGBM': lightgbm_model,\n    'CatBoost': catboost_model\n}\n\n# Evaluate each model and store the accuracies\naccuracies = {}\nfor name, model in models.items():\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracies[name] = accuracy\n\n# Create a DataFrame to store accuracies\naccuracy_df = pd.DataFrame(accuracies.items(), columns=['Model', 'Accuracy'])\n\n# Sort the DataFrame by Accuracy in descending order\naccuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False)\n\n# Display the DataFrame\naccuracy_df","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:38.063539Z","iopub.execute_input":"2024-04-02T22:47:38.063923Z","iopub.status.idle":"2024-04-02T22:47:38.735223Z","shell.execute_reply.started":"2024-04-02T22:47:38.063893Z","shell.execute_reply":"2024-04-02T22:47:38.734087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#xgboost_model_tunning \n\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n\n# Define XGBoost model\nxgboost_model = xgb.XGBClassifier()\n\n# Define parameters for hyperparameter tuning\nparameters3 = {\n    \"n_estimators\": [50, 100, 150],\n    \"random_state\": [0, 42, 50],\n    \"learning_rate\": [0.1, 0.3, 0.5, 1.0]\n}\nparams_XGB_best ={'lambda': 3.0610042624477543, \n             'alpha': 4.581902571574289, \n             'colsample_bytree': 0.9241969052729379, \n             'subsample': 0.9527591724824661, \n             'learning_rate': 0.06672065863100594, \n             'n_estimators': 725, #initial value is 651\n             'max_depth': 5, \n             'min_child_weight': 1, \n             'num_parallel_tree': 1}\n# Perform GridSearchCV\ngrid_search3 = GridSearchCV(xgboost_model, parameters3, cv=5, n_jobs=-1)\ngrid_search3.fit(X, y)\n\n# Get the best score\nbest_score = grid_search3.best_score_\n\n# Get the best parameters\nbest_parameters = grid_search3.best_params_\n\n# Create a new XGBoost model with the best parameters\nxgboost_model_tuned = xgb.XGBClassifier(**params_XGB_best)\nxgboost_model_tuned.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred3 = xgboost_model_tuned.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred3)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:47:38.736590Z","iopub.execute_input":"2024-04-02T22:47:38.736973Z","iopub.status.idle":"2024-04-02T22:48:45.845185Z","shell.execute_reply.started":"2024-04-02T22:47:38.736945Z","shell.execute_reply":"2024-04-02T22:48:45.843853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:48:45.846960Z","iopub.execute_input":"2024-04-02T22:48:45.848844Z","iopub.status.idle":"2024-04-02T22:48:45.857295Z","shell.execute_reply.started":"2024-04-02T22:48:45.848783Z","shell.execute_reply":"2024-04-02T22:48:45.856044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'num_leaves': [20, 30, 40],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'n_estimators': [50, 100, 200]\n}\n\ngrid_search = GridSearchCV(lightgbm_model, param_grid, cv=3, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\nbest_params = grid_search.best_params_\nlightgbm_model_tuned = grid_search.best_estimator_\n\ntest_score = lightgbm_model_tuned.score(X_test, y_test)\nprint(\"Best hyperparameters:\", best_params)\nprint(\"Test set accuracy:\", test_score)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:48:45.858883Z","iopub.execute_input":"2024-04-02T22:48:45.859792Z","iopub.status.idle":"2024-04-02T22:49:09.125076Z","shell.execute_reply.started":"2024-04-02T22:48:45.859749Z","shell.execute_reply":"2024-04-02T22:49:09.123840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#catboost_model tunning \nparameters2 = {\"learning_rate\":[0.1,0.3,0.5,0.6,0.7],\n              \"random_state\":[0,42,48,50],\n               \"depth\":[8,9,10],\n               \"iterations\":[35,40,50]}\n\n# Perform GridSearchCV\ngrid_search3 = GridSearchCV(catboost_model, parameters3, cv=5, n_jobs=-1)\ngrid_search3.fit(X_train, y_train)\n\n# Get the best score\nbest_score = grid_search3.best_score_\n\n# Get the best parameters\nbest_parameters = grid_search3.best_params_\n\n# Create a new XGBoost model with the best parameters\ncatboost_model_tuned = CatBoostClassifier(verbose=False, **best_parameters)\n\ncatboost_model_tuned.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_predict = catboost_model_tuned.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_predict)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:49:09.126538Z","iopub.execute_input":"2024-04-02T22:49:09.127053Z","iopub.status.idle":"2024-04-02T22:50:46.955064Z","shell.execute_reply.started":"2024-04-02T22:49:09.127019Z","shell.execute_reply":"2024-04-02T22:50:46.953622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'learning_rate': [0.05, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 4, 5]\n}\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(gradientboost_model, param_grid, cv=3, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters and best estimator\nbest_params = grid_search.best_params_\ngradientboost_model_tuned = grid_search.best_estimator_\n\n# Calculate the test set accuracy\ntest_score = gradientboost_model_tuned.score(X_test, y_test)\n\n# Print the results\nprint(\"Best hyperparameters:\", best_params)\nprint(\"Test set accuracy:\", test_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:50:46.956694Z","iopub.execute_input":"2024-04-02T22:50:46.957233Z","iopub.status.idle":"2024-04-02T22:53:07.057815Z","shell.execute_reply.started":"2024-04-02T22:50:46.957188Z","shell.execute_reply":"2024-04-02T22:53:07.056215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\n# 'Random Forest': randomforest_model,\n\n#stacking models \nstacking_model = StackingClassifier(estimators=[('LightGBM', lightgbm_model_tuned), \n                                                ('CatBoost', catboost_model_tuned),\n                                                (\"XGBoost\", xgboost_model_tuned),\n                                                ('Gradient Boost', gradientboost_model_tuned),\n                                                ('AdaBoost', adaboost_model)\n                                                ])\n\nstacking_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = stacking_model.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred) \n\nprint(\"Accuracy:\", accuracy) \n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:53:07.063273Z","iopub.execute_input":"2024-04-02T22:53:07.063921Z","iopub.status.idle":"2024-04-02T22:53:53.438735Z","shell.execute_reply.started":"2024-04-02T22:53:07.063877Z","shell.execute_reply":"2024-04-02T22:53:53.437350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>4. Model Tuning:</h1>","metadata":{}},{"cell_type":"markdown","source":"* Using techniques like grid search or random search to find the optimal hyperparameters for the selected models.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy*100","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:53:53.440049Z","iopub.execute_input":"2024-04-02T22:53:53.440432Z","iopub.status.idle":"2024-04-02T22:53:53.447199Z","shell.execute_reply.started":"2024-04-02T22:53:53.440401Z","shell.execute_reply":"2024-04-02T22:53:53.446200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>5. Model Evaluation:</h1>","metadata":{}},{"cell_type":"markdown","source":"* Evaluating the tuned models on the validation set using appropriate metrics (e.g., accuracy, precision, recall, F1-score).\n* Choosing the best-performing model based on the evaluation metrics.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\n# Perform cross-validation predictions\ncross_val_preds = cross_val_predict(stacking_model, X_test, y_test, cv=5)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, cross_val_preds) \n\nprint(\"Accuracy:\", accuracy) ","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:53:53.448616Z","iopub.execute_input":"2024-04-02T22:53:53.449212Z","iopub.status.idle":"2024-04-02T22:55:06.093360Z","shell.execute_reply.started":"2024-04-02T22:53:53.449179Z","shell.execute_reply":"2024-04-02T22:55:06.092088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>6. Predictions:</h1>","metadata":{}},{"cell_type":"markdown","source":"* Making predictions using the selected model on the test data.\n","metadata":{}},{"cell_type":"code","source":"test_pred = xgboost_model_tuned.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:55:06.095672Z","iopub.execute_input":"2024-04-02T22:55:06.096357Z","iopub.status.idle":"2024-04-02T22:55:06.167249Z","shell.execute_reply.started":"2024-04-02T22:55:06.096317Z","shell.execute_reply":"2024-04-02T22:55:06.165881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Preparing the submission file in the specified format (PassengerId,Transported) with predictions for the test set.","metadata":{}},{"cell_type":"code","source":"# Sample submission (to get right format)\nsub=pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')\n\n# Add predictions\nsub['Transported']=test_pred\n\n# Replace 0 to False and 1 to True\nsub=sub.replace({0:False, 1:True})\n\n# Prediction distribution\nplt.figure(figsize=(6,6))\nsub['Transported'].value_counts().plot.pie(explode=[0.1,0.1], autopct='%1.1f%%', shadow=True, textprops={'fontsize':16}).set_title(\"Prediction distribution\")","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:55:06.168754Z","iopub.execute_input":"2024-04-02T22:55:06.169360Z","iopub.status.idle":"2024-04-02T22:55:06.370219Z","shell.execute_reply.started":"2024-04-02T22:55:06.169323Z","shell.execute_reply":"2024-04-02T22:55:06.368896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output to csv\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T22:55:06.371980Z","iopub.execute_input":"2024-04-02T22:55:06.373013Z","iopub.status.idle":"2024-04-02T22:55:06.392456Z","shell.execute_reply.started":"2024-04-02T22:55:06.372966Z","shell.execute_reply":"2024-04-02T22:55:06.391332Z"},"trusted":true},"execution_count":null,"outputs":[]}]}